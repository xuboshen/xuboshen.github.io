<!DOCTYPE html>
<html>

<head lang="en">
  <meta charset="UTF-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">

  <title>POV</title>

  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="css/main.css">
  <link rel="icon" href="images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
  <script src="js/main.js"></script>
</head>

<body>
<div class="container" id="main">
  <div class="row">
    <h1 class="col-md-12 text-center">
      (MM '23) POV: Prompt-Oriented View-Agnostic Learning for <br> Egocentric Hand-Object Interaction in the Multi-View World<br>
    </h1>
    <!-- <venue class="col-md-12 text-center">
      <font size="5"> <b>ACM Multimedia 2023 </b></font>
    </venue> -->

  </div>
  <div class="row meta"> 
    <div class="col-md-12 text-center">
      <ul class="list-inline" style="padding-top: 10px; padding-bottom: 5px">
        <li>
          <!-- <a href="https://xuboshen.github.io/"></a> -->
          <font size="4"> Boshen Xu </font>
        </li>
        <li>
          <font size="4"> Sipeng Zheng </font>
        </li>
        <li>
          <font size="4"> Qin Jin</font> <!-- <sup>&dagger;</sup>-->
        </li>
      </ul>
      <!-- <ul style="padding-top: 0px; padding-bottom: 0px">
        <font size="4"> <sup>&dagger;</sup>: Corresponding author</font>
      </ul> -->
      <ul style="padding-top: 0px; padding-bottom: 0px">
        <font size="4"> Renmin University of China </font>
      </ul>
    </div>
  </div>


  <div class="row meta">
    <div class="col-md-4 col-md-offset-4 text-center">
      <ul class="nav nav-pills nav-justified">
        <li>
          <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">
            <image src="images/pdf.png" height="50px"></image>
            <br>
            <strong>MM'23</strong>
          </a>
        </li>
<!--        <li>-->
<!--          <a href="https://youtu.be/GwQhnG6krCI">-->
<!--            <image src="images/youtube.png" height="50px"></image>-->
<!--            <br>-->
<!--            <strong>Video</strong>-->
<!--          </a>-->
<!--        </li>-->
        <li>
          <a href="https://github.com/xuboshen/pov_acmmm2023">
            <image src="images/github.png" height="50px"></image>
            <br>
            <strong>Code</strong>
          </a>
        </li>
      </ul>
    </div>
  </div>


  <div class="row">
    <div class="col-md-8 col-md-offset-2 text-center">
      <h2>
        Abstract
      </h2>
      <p class="text-justify">
        We humans are good at translating third-person observations of
hand-object interactions (HOI) into an egocentric view. However,
current methods struggle to replicate this ability of view adaptation
from third-person to first-person. Although some approaches attempt to learn view-agnostic representation from large-scale video
datasets, they ignore the relationships among multiple third-person
views. To this end, we propose a Prompt-Oriented View-agnostic
learning (POV) framework in this paper, which enables this view
adaptation with few egocentric videos. Specifically, We introduce
interactive masking prompts at the frame level to capture fine-grained action information, and view-aware prompts at the token
level to learn view-agnostic representation. To verify our method,
we establish two benchmarks for transferring from multiple third-person views to the egocentric view. Our extensive experiments
on these benchmarks demonstrate the efficiency and effectiveness
of our POV framework and prompt tuning techniques in terms of
view adaptation and view generalization.
      </p>

      <table style="margin: 20px">
        <tr>
          <td>
            <image src="images/motivation.png" class="img-responsive center-block" style="max-width: 60%;"></image>
          </td>
        </tr>
      </table>


    </div>
  </div>


<!--  <div class="row">-->
<!--    <div class="col-md-8 col-md-offset-2">-->
<!--      <h2>-->
<!--        Video-->
<!--      </h2>-->
<!--      <div class="text-center">-->
<!--        <div style="position:relative;padding-top:56.5%;">-->
<!--          <iframe src="https://www.youtube.com/embed/GwQhnG6krCI" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->


  <div class="row">
    <div class="col-md-8 col-md-offset-2">
      <h2>
        Method
      </h2>
      <h3>
        POV Framework
      </h3>
      <p>
        Our prompt-oriented view-agnostic learning framework, which trains a model through two optimization
sub-tasks and one optional sub-task: (1) <b>prompt-based action understanding</b>, which incorporates interactive masking prompts
into frames to pre-train the entire model on third-person videos; (2) <b>view-agnostic prompt tuning</b>, where only view-aware
prompts are fine-tuned through cross-view alignment and cross-entropy loss. (3) <b>egocentric fine-tuning</b>, where the model is
optionally fine-tuned on limited egocentric videos.
      <figure>
        <image src="images/arch.png" class="img-responsive center-block" style="max-width: 100%;"></image>
      </figure>
      

      <h3>
        Specific Pipeline
      </h3>
        The training and inference pipeline of our POV on Assembly101 and its view split:
      <figure>
        <image src="images/assembly_pipeline.png" class="img-responsive center-block" style="max-width: 100%;"></image>
      </figure>

      <p>
        The training and inference pipeline of our POV on H2O and its view split:
      </p>
      <figure>
        <image src="images/h2o_pipeline.png" class="img-responsive center-block" style="max-width: 100%;"></image>
      </figure>
      
    </div>
  </div>


  <div class="row">
    <div class="col-md-8 col-md-offset-2">
      <h2>
        Experiment
      </h2>
      <h3>
        Quantatitive Results
      </h3>
      <p class="text-justify">
        We evaluate our POV framework on two benchmarks, Assembly101 and H2O, which are established for view adaptation and view generalization, respectively. Table 1 and Table 2 shows the view adaptation results of our POV framework on Assembly101 and H2O. We can see that our POV framework achieves the best performance on both benchmarks.
      </p>
      <p>
        After fine-tuning on unlabeled egocentric videos, the results of our POV on Assembly101 and H2O datasets:
      </p>
      <figure>
        <image src="images/table1.png" class="img-responsive center-block" style="max-width: 97%;"></image>
      </figure>
      <p>
        After fine-tuning on labeled egocentric videos, the results of our POV on Assembly101 and H2O datasets:
      </p>
      <figure>
        <image src="images/table2.png" class="img-responsive center-block" style="max-width: 50%;"></image>
      </figure>
      <p>
        Table 3 and Table 4 shows the view generalization results on egocentric view an third-person view, respectively. We can see that our POV framework achieves promising performance on both benchmarks.
      </p>
      <figure>
        <image src="images/table3.png" class="img-responsive center-block" style="max-width: 80%;"></image>
      </figure>
      <figure>
        <image src="images/table4.png" class="img-responsive center-block" style="max-width: 50%;"></image>
      </figure>


      <h3>
        Quanlitative results
        <figure>
          <image src="images/ass_quanlitative_result.png" class="img-responsive center-block" style="max-width: 100%;"></image>
        </figure>
        <figure>
          <image src="images/h2o_quanlitative_result.png" class="img-responsive center-block" style="max-width: 100%;"></image>
        </figure>
  
      </h3>
    </div>
  </div>


  <div class="row">
    <div class="col-md-9 col-md-offset-2">
      <h2>
        Citation
      </h2>
      <div class="form-group col-md-10 col-md-offset-0">
        <textarea id="bibtex" class="form-control" readonly>
@InProceedings{yin2022fishermatch,
  author={Yin, Yingda and Wang, Yang and Wang, He and Chen, Baoquan},
  title={A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
}</textarea>
      </div>
    </div>
  </div>




  <div class="row">
    <div class="col-md-8 col-md-offset-2">
        The website template was borrowed from <a href="https://github.com/jonbarron/website">Jon's website</a>.
    </div>
  </div>
</div>
<br>
</body>
</html>