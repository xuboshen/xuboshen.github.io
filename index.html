<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Boshen Xu</title>

    <meta name="author" content="Boshen Xu">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Boshen Xu
                </p>
                <h2>About</h2>
                <p> My research interests include video understanding (e.g., action recognition, vision-language pretraining, etc.) and embodied AI (e.g., 3D object assembly, 3D HOI reconstruction, etc.). I'm currently focus on egocentric vision and related topics that benefit VR/AR/Embodied AI.
                  </p>
                <p>I am a second-year PhD student at <a href="http://info.ruc.edu.cn/index.htm">Renmin University of China (RUC)</a> under the supervision of Professor <a href="https://www.jin-qin.com/">Qin Jin</a> at <a href="https://www.ruc-aim3.com/">AIM3 Lab</a>.
                  Prior to joining RUC, I got my bachelor degree from <a href="https://www.scse.uestc.edu.cn/">School of Computer Science and Engineering</a>, <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a>.
                  I got a GPA 3.97/4.00(90.41/100), ranked 2/65 in the major.
                </p>
                <p style="text-align:center">
                  <a href="mailto:boshenx@ruc.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/BoshenXu-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/BoshenXu-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/xuboshen/">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=K2BnqoIAAAAJ&hl=zh-CN">Google Scholar</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/xuboshen"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xuboshen.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                * denotes equal contributions.
                <!-- <p>
                  I'm interested in 2D vision, 3D vision and embodied AI. 
                  Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          



            <!-- ICLR2025 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/framework_egonce_plus.png" alt="clean-usnob" width="248" height="113.3 align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://openreview.net/pdf?id=M8gXSFGkn2"> Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions? </a></span>
                <!-- </a> -->
                <br>
                <strong>Boshen Xu</strong>, <a href="https://openreview.net/profile?id=~Ziheng_Wang5">Ziheng Wang*</a>, <a href="https://openreview.net/profile?id=~Yang_Du6">Yang Du*</a> , Zhinan Song, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin</a>
                <br>
                <em>ICLR</em>, 2025
                <br>
                <!-- <a href="https://xuboshen.github.io/POV/">project page</a> -->
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                <!-- / -->
                <a href="https://github.com/xuboshen/EgoNCEpp">code</a>
                /
                <a href="https://openreview.net/pdf?id=M8gXSFGkn2">paper</a>

                <p></p>
                <p>We propose EgoNCE++, an asymmetric contrastive learning pretraining objective to solve the EgoVLMs' weakness in distinguishing HOI combinations with word variations.</p>
              </td>
            </tr>
            <!--/ ICLR2025 -->

            <!-- 3DV 2025 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_SPAFormer.png" alt="clean-usnob" width="248" height="219" align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://arxiv.org/abs/2403.05874"> SPAFormer: Sequential 3D Part Assembly with Transformers</a></span>
                <!-- </a> -->
                <br>
                <strong>Boshen Xu</strong>, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin</a>
                <br>
                <em>3DV</em>, 2025
                <br>
                <a href="https://xuboshen.github.io/SPAFormer/">project page</a>
                /
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                <!-- / -->
                <a href="https://github.com/xuboshen/SPAFormer">code</a>
                /
                <a href="https://arxiv.org/abs/2403.05874">arxiv</a>

                <p></p>
                <p>We present SPAFormer, a transformer-based framework that leverages assembly sequences constraints with three part encodings to address the combinatorial explosion challenge in 3D-PA task. 
                  <!-- , effectively achieving promising results on our extended benchmark PartNet-Assembly.  -->
              </td>
            </tr>
            <!--/ 3DV 2025 -->

            <!-- ECCV Workshop 2024 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_unvealing.png" alt="clean-usnob" width="248" height="113.3 align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://arxiv.org/pdf/2409.06709"> Unveiling Visual Biases in Audio-Visual Localization Benchmarks </a></span>
                <!-- </a> -->
                <br>
                <a href="https://openreview.net/profile?id=~Liangyu_Chen5">Liangyu Chen</a>, <a href="https://yuezih.github.io/">Zihao Yue</a>, <strong>Boshen Xu</strong>, <a href="http://www.jin-qin.com/">Qin Jin</a>
                <br>
                <em>ECCV AVGenL Workshop</em>, 2024
                <br>
                <!-- <a href="https://xuboshen.github.io/POV/">project page</a> -->
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                <!-- / -->
                <a href="https://arxiv.org/pdf/2409.06709">arxiv</a>

                <p></p>
                <p> We reveal that current audio-visual source localization benchmarks (VGG-SS, Epic-Sounding-Object) are easily hacked by vision-only models, therefore calling for a benchmark that requires more audio cues.</p>
              </td>
            </tr>
            <!--/ ECCV Workshop 2024 -->

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_POV.png" alt="clean-usnob" width="248" height="219" align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://dl.acm.org/doi/10.1145/3581783.3612484"> Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World</a></span>
                <!-- </a> -->
                <br>
                <strong>Boshen Xu</strong>, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin</a>
                <br>
                <em>ACM MM</em>, 2023
                <br>
                <a href="https://xuboshen.github.io/POV/">project page</a>
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                /
                <a href="https://github.com/xuboshen/pov_acmmm2023">code</a>
                /
                <a href="https://arxiv.org/abs/2403.05856">arxiv</a>

                <p></p>
                <p>We propose POV, a view adaptation framework that enables transfer learning from multi-view third-person videos to egocentric videos.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_OpenCat.png" alt="clean-usnob" width="248" height="160">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf">Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework</a></span>
                <!-- </a> -->
                <br>
                <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <strong>Boshen Xu</strong>, <a href="http://www.jin-qin.com/">Qin Jin</a>
                <br>
                <em>CVPR</em>, 2023
                <p>We introduce
                  OpenCat, a language modeling framework that reformulates HOI prediction as sequence generation.</p>
              </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Awards</h2>
              
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                  <ul>
                    <li>2023, Outstanding Graduate, Sichuan, China</li>
                    <li>2021, Tencent Special Scholarship, UESTC & Tencent</li>
                    <li>2021, Second Prize of China Undergraduate Mathematical Contest in Modeling, China</li>
                    <li>2020, National Scholarship, UESTC, China</li>
                  </ul>

                </tbody></table>
              <h2>Services</h2>
              
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                  <ul>
                    <li>Conference Reviewer for ICLR, ACM MM, ACCV.</li>
                  </ul>
                  <ul>
                    <li>Teaching Assistant for Multimedia Application Technology (RUC 2024 Fall).</li>
                  </ul>

                </tbody></table>

            </td>
          </tr>
        </tbody></table>

<!--           
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
             -->
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/xuboshen/xuboshen.github.io">template</a>. Inspired by <a href="https://github.com/jonbarron/jonbarron_website">Jon's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>