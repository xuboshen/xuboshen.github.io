<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Boshen Xu</title>

    <meta name="author" content="Boshen Xu">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Boshen Xu
                </p>
                <h2>About</h2>
                <p> My research interests include video understanding (e.g., action recognition, vision-language pretraining, long-form video grounding, etc.) and embodied AI (e.g., 3D object assembly, 3D HOI reconstruction, etc.). I'm currently focusing on long video understanding and multi-modal large language models.
                  </p>
                <p>I am a third-year PhD student at <a href="http://info.ruc.edu.cn/index.htm">Renmin University of China (RUC)</a> under the supervision of Professor <a href="https://www.jin-qin.com/">Qin Jin</a> at <a href="https://www.ruc-aim3.com/">AIM3 Lab</a>.
                  Prior to joining RUC, I got my bachelor degree from <a href="https://www.scse.uestc.edu.cn/">School of Computer Science and Engineering</a>, <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a>.
                  
                </p>
                <p style="text-align:center">
                  <a href="mailto:boshenx@ruc.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/BoshenXu-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/BoshenXu-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/xuboshen/">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=K2BnqoIAAAAJ&hl=zh-CN">Google Scholar</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/xuboshen"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xuboshen.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                *: equal contributions. &dagger;: corresponding author. &ddagger;: project lead.
                <!-- <p>
                  I'm interested in 2D vision, 3D vision and embodied AI. 
                  Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
            <!-- Time-R1 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/TimeR1_comparison.png" alt="Time-R1" width="248" height="134.72" align="center">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">
                  <a href="https://arxiv.org/abs/2503.13377v3">Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding</a>
                </span>
                <br>
                <a href="https://openreview.net/profile?id=~Ye_Wang12">Ye Wang*</a>, <a href="https://openreview.net/profile?id=~Ziheng_Wang5">Ziheng Wang*</a>, <strong>Boshen Xu*<sup style="font-size: 0.6em;">&ddagger;</sup></strong>, <a href="https://openreview.net/profile?id=~Yang_Du6">Yang Du</a>, <a href="https://openreview.net/profile?id=~Kejun_Lin1">Kejun Lin</a>, <a href="https://openreview.net/profile?id=~Zihan_Xiao1">Zihan Xiao</a>,
                <a href="https://yuezih.github.io/">Zihao Yue</a>, <a href="https://openreview.net/profile?id=~Jianzhong_Ju1">Jianzhong Ju</a>, <a href="https://openreview.net/profile?id=~Liang_Zhang10">Liang Zhang</a>, <a href="https://openreview.net/profile?id=~Dingyi_Yang1">Dingyi Yang</a>, Xiangnan Fang, Zewen He, 
                <a href="https://openreview.net/profile?id=~Zhenbo_Luo2">Zhenbo Luo</a>, <a href="https://jarviswang94.github.io/">Wenxuan Wang</a>, Junqi Lin, <a href="https://openreview.net/profile?id=~Jian_Luan1">Jian Luan</a>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>NeurIPS</em>, 2025
                <br>
                <a href="https://xuboshen.github.io/Time-R1/">project page</a> /
                <a href="https://github.com/xiaomi-research/time-r1">code</a> /
                <a href="https://arxiv.org/abs/2503.13377v2">arxiv</a>
                <p></p>
                <p>We introduce <strong>Time-R1</strong> framework, <strong>TimeRFT</strong> training, and <strong>TVGBench</strong> for LVLM evaluation, to advance the field of using LVLM for temporal video grounding. 
                  Time-R1 achieves state-of-the-art performance on TVG using only 2.5K data for RL fine-tuning, with improved performance on 4 VQA benchmarks.  </p>
              </td>
            </tr>
            <!--/ Time-R1 -->

            <!-- EgoDTM -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/EgoDTM_comparison.png" alt="EgoDTM" width="248" height="138.30" align="center">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">
                  <a href="http://arxiv.org/abs/2503.15470">EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining</a>
                </span>
                <br>
                <strong>Boshen Xu</strong>, <a href="https://openreview.net/profile?id=~Yuting_Mei2">Yuting Mei</a>, <a href="https://openreview.net/profile?id=~liu_xinbi1">Xinbi Liu</a>, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>NeurIPS</em>, 2025
                <br>
                <a href="https://github.com/xuboshen/EgoDTM">code</a> /
                <a href="http://arxiv.org/abs/2503.15470">arxiv</a>
                <p></p>
                <p>We introduce <strong>EgoDTM</strong>, an Egocentric Depth- and Text-aware Model that bridges the gap between 2D visual understanding and 3D spatial awareness. </p>
              </td>
            </tr>
            <!--/ EgoDTM -->

            <!-- timezero -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/TimeZero_comps.png" alt="TimeZero" width="248" height="155.57" align="center">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">
                  <a href="https://arxiv.org/abs/2503.13377v1">TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM</a>
                </span>
                <br>
                <a href="https://openreview.net/profile?id=~Ye_Wang12">Ye Wang*</a>, <strong>Boshen Xu*</strong>, <a href="https://yuezih.github.io/">Zihao Yue</a>, <a href="https://openreview.net/profile?id=~Zihan_Xiao1">Zihan Xiao</a> , <a href="https://openreview.net/profile?id=~Ziheng_Wang5">Ziheng Wang</a>, 
                <a href="https://openreview.net/profile?id=~Liang_Zhang10">Liang Zhang</a> , <a href="https://openreview.net/profile?id=~Dingyi_Yang1">Dingyi Yang</a>, <a href="https://jarviswang94.github.io/">Wenxuan Wang</a>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>CVPRW</em>, 2025
                <br>
                <a href="https://github.com/www-Ye/TimeZero">code</a> /
                <a href="http://arxiv.org/abs/2503.13377v1">paper</a>
                <p></p>
                <p>We propose <strong>TimeZero</strong>, a reasoning-guided LVLM for temporal video grounding that extends inference through reinforcement learning to reason about video-language relationships. Achieves state-of-the-art performance on Charades-STA benchmark.</p>
              </td>
            </tr>
            <!--/ timezero -->

            <!-- ICLR2025 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/framework_egonce_plus.png" alt="clean-usnob" width="248" height="113.3 align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://openreview.net/pdf?id=M8gXSFGkn2"> Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions? </a></span>
                <!-- </a> -->
                <br>
                <strong>Boshen Xu</strong>, <a href="https://openreview.net/profile?id=~Ziheng_Wang5">Ziheng Wang*</a>, <a href="https://openreview.net/profile?id=~Yang_Du6">Yang Du*</a> , Zhinan Song, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>ICLR</em>, 2025
                <br>
                <!-- <a href="https://xuboshen.github.io/POV/">project page</a> -->
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                <!-- / -->
                <a href="https://github.com/xuboshen/EgoNCEpp">code</a>
                /
                <a href="https://openreview.net/pdf?id=M8gXSFGkn2">paper</a>
                /
                <a href="storage/learning_to_perceive_EgoHOI-xbs-nus.pdf">slides</a>
                <p></p>
                <p>We propose <strong>EgoNCE++</strong>, an asymmetric contrastive learning pretraining objective to solve the EgoVLMs' weakness in distinguishing HOI combinations with word variations.</p>
              </td>
            </tr>
            <!--/ ICLR2025 -->

            <!-- 3DV 2025 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_SPAFormer.png" alt="clean-usnob" width="248" height="219" align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://arxiv.org/abs/2403.05874"> SPAFormer: Sequential 3D Part Assembly with Transformers</a></span>
                <!-- </a> -->
                <br>
                <strong>Boshen Xu</strong>, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>3DV</em>, 2025
                <br>
                <a href="https://xuboshen.github.io/SPAFormer/">project page</a>
                /
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                <!-- / -->
                <a href="https://github.com/xuboshen/SPAFormer">code</a>
                /
                <a href="https://arxiv.org/abs/2403.05874">arxiv</a>

                <p></p>
                <p>We present <strong>SPAFormer</strong>, a transformer-based framework that leverages assembly sequences constraints with three part encodings to address the combinatorial explosion challenge in 3D-PA task. 
                  <!-- , effectively achieving promising results on our extended benchmark PartNet-Assembly.  -->
              </td>
            </tr>
            <!--/ 3DV 2025 -->

            <!-- ECCV Workshop 2024 -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_unvealing.png" alt="clean-usnob" width="248" height="113.3 align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://arxiv.org/pdf/2409.06709"> Unveiling Visual Biases in Audio-Visual Localization Benchmarks </a></span>
                <!-- </a> -->
                <br>
                <a href="https://openreview.net/profile?id=~Liangyu_Chen5">Liangyu Chen</a>, <a href="https://yuezih.github.io/">Zihao Yue</a>, <strong>Boshen Xu</strong>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>ECCV AVGenL Workshop</em>, 2024
                <br>
                <!-- <a href="https://xuboshen.github.io/POV/">project page</a> -->
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                <!-- / -->
                <a href="https://arxiv.org/pdf/2409.06709">arxiv</a>

                <p></p>
                <p> We reveal that current audio-visual source localization benchmarks (VGG-SS, Epic-Sounding-Object) are easily hacked by vision-only models, therefore calling for a benchmark that requires more audio cues.</p>
              </td>
            </tr>
            <!--/ ECCV Workshop 2024 -->

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_POV.png" alt="clean-usnob" width="248" height="219" align="center">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://dl.acm.org/doi/10.1145/3581783.3612484"> Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World</a></span>
                <!-- </a> -->
                <br>
                <strong>Boshen Xu</strong>, <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>ACM MM</em>, 2023
                <br>
                <a href="https://xuboshen.github.io/POV/">project page</a>
                <!-- /
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612484">paper</a> -->
                /
                <a href="https://github.com/xuboshen/pov_acmmm2023">code</a>
                /
                <a href="https://arxiv.org/abs/2403.05856">arxiv</a>

                <p></p>
                <p>We propose <strong>POV</strong>, a view adaptation framework that enables transfer learning from multi-view third-person videos to egocentric videos.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/motivation_OpenCat.png" alt="clean-usnob" width="248" height="160">
              </td>
              <td width="75%" valign="middle">
                <!-- <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"> -->
                  <span class="papertitle"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf">Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework</a></span>
                <!-- </a> -->
                <br>
                <a href="http://zhengsipeng.github.io/">Sipeng Zheng</a>, <strong>Boshen Xu</strong>, <a href="http://www.jin-qin.com/">Qin Jin<sup style="font-size: 0.6em;">&dagger;</sup></a>
                <br>
                <em>CVPR</em>, 2023
                <p>We introduce
                  <strong>OpenCat</strong>, a language modeling framework that reformulates HOI prediction as sequence generation.</p>
              </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Awards</h2>
              
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                  <ul>
                    <li>2023, Outstanding Graduate, Sichuan, China</li>
                    <li>2021, Tencent Special Scholarship, UESTC & Tencent</li>
                    <li>2021, Second Prize of China Undergraduate Mathematical Contest in Modeling, China</li>
                    <li>2020, National Scholarship, UESTC, China</li>
                  </ul>

                </tbody></table>
              <h2>Services</h2>
              
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                  <ul>
                    <li>Conference Reviewer for CVPR, NeurIPS, ICLR, ACM MM, ACL, ACCV.</li>
                  </ul>
                  <ul>
                    <li>Journal Reviewer for TOMM.</li>
                  </ul>
                  <ul>
                    <li>Teaching Assistant for Multimedia Application Technology (RUC 2024 Fall).</li>
                  </ul>

                </tbody></table>

            </td>
          </tr>
        </tbody></table>

<!--           
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
             -->
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/xuboshen/xuboshen.github.io">template</a>. Inspired by <a href="https://github.com/jonbarron/jonbarron_website">Jon's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>